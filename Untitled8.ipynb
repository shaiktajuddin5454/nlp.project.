{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CNN for Text Classification (Colab-ready mini-project)\n",
        "File: CNN_Text_Classification_Colab.py\n",
        "\n",
        "How to use:\n",
        "1. Open Google Colab (https://colab.research.google.com)\n",
        "2. New Python 3 notebook -> Runtime -> Change runtime type -> GPU (optional)\n",
        "3. Copy-paste the contents of this file into a single code cell (or upload as .py and run)\n",
        "\n",
        "This script provides two dataset options:\n",
        "- Option A (default, recommended): use TensorFlow / Keras built-in IMDB dataset (no Kaggle credentials required)\n",
        "- Option B: use Kaggle \"IMDB Dataset of 50k\" (shows commands to set up `kaggle.json`) — uncomment if you want to use Kaggle\n",
        "\n",
        "The model is a compact, well-documented 1D-CNN for binary sentiment classification.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ---------- 0. Environment check (Colab-friendly) ----------\n",
        "# If running in Colab, you already have most libs. Below installs are safe to run locally/Colab.\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "except Exception:\n",
        "    !pip install -q tensorflow\n",
        "    import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Optional: use GPU if available (Colab -> Runtime -> Change runtime type -> GPU)\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "        print('GPU found and configured')\n",
        "    except Exception as e:\n",
        "        print('GPU found but could not set memory growth:', e)\n",
        "else:\n",
        "    print('No GPU found. CPU will be used.')\n",
        "\n",
        "# ---------- 1. Select dataset source ----------\n",
        "# OPTION A (recommended): built-in Keras IMDB dataset (already tokenized as integer sequences)\n",
        "# OPTION B (more realistic): use raw text CSV (Kaggle IMDB 50k). If you want Kaggle, follow the instructions below.\n",
        "USE_KAGGLE_CSV = False\n",
        "\n",
        "# If you set USE_KAGGLE_CSV = True, follow these steps in Colab (example):\n",
        "# 1) Upload your kaggle.json (from your Kaggle account) to the Colab session (Files -> Upload)\n",
        "# 2) Uncomment the block below to move kaggle.json, set permissions, and download the dataset\n",
        "\n",
        "if USE_KAGGLE_CSV:\n",
        "    # !mkdir -p ~/.kaggle\n",
        "    # !cp kaggle.json ~/.kaggle/\n",
        "    # !chmod 600 ~/.kaggle/kaggle.json\n",
        "    # !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "    # !unzip -o imdb-dataset-of-50k-movie-reviews.zip\n",
        "    # df = pd.read_csv('IMDB Dataset.csv')\n",
        "    # df.head()\n",
        "    raise RuntimeError('Set USE_KAGGLE_CSV = False or follow the Kaggle setup commands in a cell in Colab.')\n",
        "\n",
        "# ---------- 2. Load dataset (Option A) ----------\n",
        "# Keras provides an IMDB dataset where words are mapped to integers. But for a CNN on raw text we often prefer raw text + tokenizer.\n",
        "# We'll use the tensorflow_datasets load for the raw text, but keep things simple and use a small custom load based on keras's raw_text_dataset_from_directory.\n",
        "\n",
        "# Simpler: download a small raw-text version using tensorflow datasets (tfds)\n",
        "USE_TFDS = False\n",
        "if USE_TFDS:\n",
        "    !pip install -q tensorflow-datasets\n",
        "    import tensorflow_datasets as tfds\n",
        "    ds_train, ds_test = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True)\n",
        "    # Convert to pandas DataFrame for consistency\n",
        "    def ds_to_df(ds):\n",
        "        texts = []\n",
        "        labels = []\n",
        "        for text, label in tfds.as_numpy(ds):\n",
        "            texts.append(text.decode('utf-8'))\n",
        "            labels.append(int(label))\n",
        "        return pd.DataFrame({'review': texts, 'sentiment': labels})\n",
        "    df_train = ds_to_df(ds_train)\n",
        "    df_test = ds_to_df(ds_test)\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "else:\n",
        "    # We'll construct a small example dataset by downloading the IMDB dataset via `tensorflow.keras.datasets` and mapping back to words\n",
        "    # But the Keras dataset is integer-encoded; thus for a realistic pipeline we'll instead fetch the 'aclImdb' raw dataset if online access allowed.\n",
        "    # For reliability, let's use a fallback: load a small demo dataset or the Kaggle CSV if the user uploads it.\n",
        "\n",
        "    # Fallback: try to read IMDB CSV if uploaded to Colab session local filesystem\n",
        "    if os.path.exists('IMDB Dataset.csv'):\n",
        "        df = pd.read_csv('IMDB Dataset.csv')\n",
        "    else:\n",
        "        # If no CSV available, download a lightweight sample from tf.keras (preprocessed integers) and create toy raw text from it.\n",
        "        # For real project, upload 'IMDB Dataset.csv' to Colab or set USE_KAGGLE_CSV=True and follow instructions.\n",
        "        from tensorflow.keras.datasets import imdb\n",
        "        print('No IMDB CSV found locally. Loading Keras IMDB integer dataset and converting to simple text (toy example).')\n",
        "        (X_train_i, y_train_i), (X_test_i, y_test_i) = imdb.load_data(num_words=10000)\n",
        "        word_index = imdb.get_word_index()\n",
        "        index_word = {index+3: word for word, index in word_index.items()}  # Keras reserves indices\n",
        "        index_word[0] = '<PAD>'\n",
        "        index_word[1] = '<START>'\n",
        "        index_word[2] = '<UNK>'\n",
        "        index_word[3] = '<UNUSED>'\n",
        "\n",
        "        def seqs_to_texts(seqs):\n",
        "            texts = []\n",
        "            for seq in seqs:\n",
        "                words = [index_word.get(i, '?') for i in seq]\n",
        "                texts.append(' '.join(words))\n",
        "            return texts\n",
        "\n",
        "        texts = seqs_to_texts(np.concatenate([X_train_i, X_test_i]))\n",
        "        labels = np.concatenate([y_train_i, y_test_i])\n",
        "        df = pd.DataFrame({'review': texts, 'sentiment': labels})\n",
        "\n",
        "print('Dataset size:', len(df))\n",
        "print(df.head())\n",
        "\n",
        "# ---------- 3. Basic EDA ----------\n",
        "print('\\nLabel distribution:')\n",
        "print(df['sentiment'].value_counts())\n",
        "print('\\nSample review (cleaned):')\n",
        "print(df['review'].iloc[0][:300])\n",
        "\n",
        "# ---------- 4. Text cleaning function ----------\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    # Remove characters that are not letters (keep spaces)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9' ]\", ' ', text)\n",
        "    # Reduce multiple spaces to single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Lowercase\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Apply cleaning (this can take time on 50k rows; it's fine in Colab)\n",
        "print('\\nCleaning text... (this may take a few seconds)')\n",
        "df['clean_review'] = df['review'].astype(str).apply(clean_text)\n",
        "\n",
        "# ---------- 5. Tokenization & Padding ----------\n",
        "# Hyperparameters — tune these for the mini-project\n",
        "VOCAB_SIZE = 15000    # number of words to keep\n",
        "MAX_LEN = 200         # max sequence length\n",
        "EMBED_DIM = 128       # embedding dimension\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_review'])\n",
        "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "y = df['sentiment'].astype(int).values\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('Example sequence (first review tokens):', X[0][:20])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
        "\n",
        "# Further split a validation set from training set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
        "print('After val split -> Train:', X_train.shape, 'Val:', X_val.shape)\n",
        "\n",
        "# ---------- 6. Build the CNN model ----------\n",
        "# A robust small CNN architecture for text classification\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, input_length=MAX_LEN),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='valid'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ---------- 7. Callbacks ----------\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    ModelCheckpoint('best_cnn_model.h5', save_best_only=True, monitor='val_loss')\n",
        "]\n",
        "\n",
        "# ---------- 8. Train the model ----------\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ---------- 9. Evaluation ----------\n",
        "print('\\nEvaluating on test set:')\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f'Test Loss: {loss:.4f} | Test Accuracy: {acc*100:.2f}%')\n",
        "\n",
        "# Predictions and classification report\n",
        "y_pred_prob = model.predict(X_test, verbose=0)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int).reshape(-1)\n",
        "\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ---------- 10. Save tokenizer and model ----------\n",
        "# Save tokenizer for later inference\n",
        "import json\n",
        "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(tokenizer.to_json())\n",
        "\n",
        "model.save('cnn_text_model.h5')\n",
        "print('\\nSaved model to cnn_text_model.h5 and tokenizer.json')\n",
        "\n",
        "# ---------- 11. Inference helper ----------\n",
        "\n",
        "def predict_sentiment(texts, model, tokenizer, maxlen=MAX_LEN):\n",
        "    cleaned = [clean_text(t) for t in texts]\n",
        "    seq = tokenizer.texts_to_sequences(cleaned)\n",
        "    pad = pad_sequences(seq, maxlen=maxlen, padding='post')\n",
        "    probs = model.predict(pad)\n",
        "    labels = ['Positive' if p >= 0.5 else 'Negative' for p in probs.reshape(-1)]\n",
        "    return list(zip(texts, probs.reshape(-1), labels))\n",
        "\n",
        "# Try example\n",
        "examples = [\n",
        "    \"This movie was fantastic! I loved the story and the acting.\",\n",
        "    \"I hated this film. It was boring and too long.\"\n",
        "]\n",
        "print('\\nExample predictions:')\n",
        "for txt, prob, lbl in predict_sentiment(examples, model, tokenizer):\n",
        "    print(f\"{lbl} (prob={prob:.3f}) -> {txt}\")\n",
        "\n",
        "# ---------- 12. Next steps & tips ----------\n",
        "# - Try varying VOCAB_SIZE, MAX_LEN, EMBED_DIM and number of Conv filters/kernel sizes\n",
        "# - Try adding multiple Conv1D with different kernel sizes (3,5,7) and concatenate (like a multi-channel CNN)\n",
        "# - Try pretrained embeddings (GloVe) by building an embedding matrix and setting Embedding(weights=[..], trainable=False or True)\n",
        "# - For multi-class tasks, change final Dense to units=num_classes and activation='softmax' with categorical_crossentropy\n",
        "# - For deployment: convert model to SavedModel, or export tokenizer + model and load in Flask/FastAPI\n",
        "\n",
        "print('\\nAll done — notebook finished.\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "RmKG-CL9jUOK",
        "outputId": "12d7857d-6112-4d73-8a72-4fe50eb2c84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "No GPU found. CPU will be used.\n",
            "No IMDB CSV found locally. Loading Keras IMDB integer dataset and converting to simple text (toy example).\n",
            "Dataset size: 50000\n",
            "                                              review  sentiment\n",
            "0  <START> this film was just brilliant casting l...          1\n",
            "1  <START> big hair big boobs bad music and a gia...          0\n",
            "2  <START> this has to be one of the worst films ...          0\n",
            "3  <START> the <UNK> <UNK> at storytelling the tr...          1\n",
            "4  <START> worst mistake of my life br br i picke...          0\n",
            "\n",
            "Label distribution:\n",
            "sentiment\n",
            "1    25000\n",
            "0    25000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample review (cleaned):\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the\n",
            "\n",
            "Cleaning text... (this may take a few seconds)\n",
            "X shape: (50000, 200)\n",
            "Example sequence (first review tokens): [  12   20   14   40  514 1055 1468 1306   65  461 4160   63 4047    2\n",
            "  174   34  254    3   23   99]\n",
            "Train shape: (40000, 200) Test shape: (10000, 200)\n",
            "After val split -> Train: (36000, 200) Val: (4000, 200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n"
          ]
        }
      ]
    }
  ]
}